{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88fd6e44",
   "metadata": {},
   "source": [
    "# Data Collection Phase\n",
    "\n",
    "You have to scrape at least 20000 rows of data. You can scrape more data as well, itâ€™s up to you. More the data better the model that gets created later. In this section you need to scrape the reviews of different Laptops, Smart Phones, Headphones, Smart Watches, Professional Cameras, Printers, Monitors, Home Theaters, Routers etc. from different e-commerce websites.\n",
    "\n",
    "Basically, we need these columns:\n",
    "1. reviews of the product.\n",
    "2. rating of the product.\n",
    "\n",
    "You can fetch other data as well, if you think data can be useful or can help in the project. It completely depends on your imagination or assumption.\n",
    "\n",
    "\n",
    "\n",
    "Hint:\n",
    "- Try to fetch data from different websites. If data is from different websites, it will help our model to remove the effect of over fitting.\n",
    "- Try to fetch an equal number of reviews for each rating, for example if you are fetching 10000 reviews then all ratings 1,2,3,4,5 should be 2000. It will balance our data set.\n",
    "- Convert all the ratings to their round number, as there are only 5 options for rating i.e., 1,2,3,4,5. If a rating is 4.5 convert it 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the neccessary libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "import  requests\n",
    "from  bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a607c",
   "metadata": {},
   "source": [
    "### Reviews from Flipkart.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\sonalidaga\\Desktop\\Ratings Prediction Project Data (Flip Robo)\\chromedriver.exe\")\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "#clicking on cancel buttopn \n",
    "driver.find_element_by_xpath(\"//div[@class='_2QfC02']/button\").click()\n",
    "srch_items = ['laptops', 'Phones','smart watches','Monitors']\n",
    "title = []\n",
    "review_text = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c202ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap():    \n",
    "        for i in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\"):\n",
    "            review_text.append(i.text)\n",
    "        for i in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\"):\n",
    "            title.append(i.text)\n",
    "        for i in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']/../div\"):\n",
    "            ratings.append(i.text)\n",
    "        return\n",
    "urls=[]\n",
    "for i in srch_items:\n",
    "    # Find the search bar\n",
    "    srchBar = driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']/input\")\n",
    "    srchBar.clear()\n",
    "    srchBar.send_keys(i)\n",
    "    # Clicking on the search button\n",
    "    driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "    time.sleep(3)\n",
    "    page = []\n",
    "    for i in driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\"):\n",
    "        page.append(i.get_attribute('href'))\n",
    "    for i in page[0:4]:\n",
    "        driver.get(i)\n",
    "        time.sleep(3)\n",
    "        items = driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "        for i in items:\n",
    "            urls.append(i.get_attribute('href'))\n",
    "len(urls)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a8e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    for _ in range(2):\n",
    "        driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "        time.sleep(1)\n",
    "    # Clicking on all reviews\n",
    "    try:\n",
    "        btn=driver.find_element_by_xpath(\"//div[@class='_2c2kV-']/following::a\")\n",
    "        lnk = btn.get_attribute('href')\n",
    "        driver.get(lnk)\n",
    "        time.sleep(1)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    scrap()        \n",
    "    try:\n",
    "        n_page=driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\")\n",
    "        np=[]\n",
    "        for i in n_page:\n",
    "            np.append(i.get_attribute('href'))\n",
    "        for i in np[0:18]:\n",
    "            driver.get(i)\n",
    "            time.sleep(1)\n",
    "            scrap()\n",
    "    except: continue\n",
    "len(ratings), len(review_text), len(title)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da657cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe\n",
    "data = list(zip(title,review_text,ratings))\n",
    "df = pd.DataFrame(data, columns = [\"Review_title\",\"Review_text\",\"Ratings\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data into csv file\n",
    "df.to_csv(r\"C:\\Users\\sonalidaga\\Desktop\\Ratings Prediction Project Data (Flip Robo)\\flipkart1_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f19744",
   "metadata": {},
   "source": [
    "### Reviews from Amazon.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "srch_items = ['laptops','phones','headphones','smart watches', 'Professional Cameras', 'Printers', \n",
    "              'Monitors', 'Home theater', 'Router']\n",
    "title = []\n",
    "review_text = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eee1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in srch_items:\n",
    "    srchbar = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    srchbar.send_keys(i)\n",
    "    driver.find_element_by_id(\"nav-search-submit-button\").click()  #clicking on search button\n",
    "    time.sleep(1)\n",
    "    item_url = []\n",
    "    for i in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        item_url.append(i.get_attribute('href'))\n",
    "    for i in item_url:\n",
    "        driver.get(i)\n",
    "        time.sleep(1)\n",
    "        for _ in range(2):\n",
    "            driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "            time.sleep(1)\n",
    "        # Clicking on see all reviews\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//a[@class='a-link-emphasis a-text-bold']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        url_ = []\n",
    "        try:\n",
    "            two_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[4]/td[2]/a\")\n",
    "            url_.append(two_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            three_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[3]/td[2]/a\")\n",
    "            url_.append(three_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            one_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[5]/td[2]/a\")\n",
    "            url_.append(one_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            fiv_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[1]/td[2]/a\")\n",
    "            url_.append(fiv_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            four_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[2]/td[2]/a\")\n",
    "            url_.append(four_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        \n",
    "        for i in url_:\n",
    "            driver.get(i)\n",
    "            time.sleep(1)\n",
    "            for i in range(10):\n",
    "                ids = driver.find_elements_by_xpath(\"//div[@class='a-section a-spacing-none review-views celwidget']/div\")\n",
    "                comment_ids = []\n",
    "                for i in ids:\n",
    "                    comment_ids.append(i.get_attribute('id'))\n",
    "\n",
    "                for x in comment_ids:\n",
    "                    # Extract user ids from each user on a page\n",
    "                    try:\n",
    "                        review_title = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[2]/a[2]/span[1]')\n",
    "                        title.append(review_title.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        title.append('')\n",
    "\n",
    "                    try:\n",
    "                        rating = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[2]/a[1]/i/span[1]')\n",
    "                        ratings.append(rating.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        ratings.append('')\n",
    "\n",
    "                    try:\n",
    "                        text = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[4]/span/span')\n",
    "                        review_text.append(text.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        review_text.append('')\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"//ul[@class='a-pagination']/li[2]/a\").click()\n",
    "                    time.sleep(3)\n",
    "                except : break \n",
    "len(ratings), len(title), len(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd40b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe\n",
    "data = list(zip(title,review_text,ratings))\n",
    "df = pd.DataFrame(data, columns = [\"Review_title\",\"Review_text\",\"Ratings\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae39ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data into csv file\n",
    "df.to_csv(r\"C:\\Users\\sonalidaga\\Desktop\\Ratings Prediction Project Data (Flip Robo)\\amazon1_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the flipkart and amazon reviews files to combine them together in a single csv file\n",
    "flipkart = pd.read_csv(r\"C:\\Users\\sonalidaga\\Desktop\\Ratings Prediction Project Data (Flip Robo)\\flipkart1_reviews.csv\")\n",
    "amazon = pd.read_csv(r\"C:\\Users\\sonalidaga\\Desktop\\Ratings Prediction Project Data (Flip Robo)\\amazon1_reviews.csv\")\n",
    "reviews = pd.concat([amazon, flipkart], ignore_index=True)\n",
    "reviews.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final data into csv file\n",
    "reviews.to_csv(r\"C:\\Users\\sonalidaga\\Desktop\\Ratings Prediction Project Data (Flip Robo)\\Review_Rating_Datafile.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
